
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script type="text/javascript" charset="utf-8" src="https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script> 
<!---
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
--->


<style type="text/css">
body {
	font-family: Google Sans, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
	font-size: 22px;
	font-weight: 400;
	color: #000;
	line-height: 1.5em;
    margin-left: auto;
    margin-right: auto;
}

@media screen and (min-width: 1200px){
    body {
        width: 1200px;
    }
}


h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #5364cc;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1 {
    text-align: center;
}
h2,h3 {
    text-align: left;
}

h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-size: 40px;
    font-weight: 20px;
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
h3 {
    font-weight: 600;
    margin: 16px 0px 4px 0px;
}

.paper-title {
    padding: 1px 0px 1px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-5 {
     width: 20%;
     float: left;
}

.move-down {
    margin-top:1.2cm;
}

.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.col-1 {
     width: 100%;
     float: left;
}

.author-row, .affil-row {
    font-size: 17px;
}

.author-row-new { 
    text-align: center; 
}

.author-row-new a {
    display: inline-block;
    font-size: 17px;
    padding: 4px;
}

.author-row-new sup {
    color: #313436;
    font-size: 13   px;
    padding: 4px;
}

.affiliations-new {
    font-size: 16px;
    text-align: center;
    width: 80%;
    margin: 0 auto;
    margin-bottom: 20px;
}

.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 26px;
}
.affiliatons {
    font-size: 18px;
}
.affil-row {
    margin-top: 18px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    color: #666;
    margin-top: 4px;
    margin-bottom: 10px;
    text-align: left;
}


video {
    display: block;
    margin: auto;
}


figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    padding: 0;
    margin: 0;
    list-style: none;
}
.flex-row-center {
    display: flex;
    flex-flow: row wrap;
    padding: 0;
    margin: 0;
    list-style: none;
    justify-content: center;
    text-align: center;
}
.flex-container {
  display: flex;
  flex-wrap: wrap;
}

.flex-item {
  flex: 0 0 50%;
  padding: 10px;
  box-sizing: border-box;
}

.paper-btn-coming-soon {
    position: relative; 
    top: 0;
    left: 0;
}

.coming-soon {
    position: absolute;
    top: -15px;
    right: -15px;
}

.center {
  margin-left: 10.0%;
  margin-right: 10.0%;
}

.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #E0F7FA;
  color: #01579B !important;
  font-size: 20px;
  width: 200px;
  font-weight: 600;
}

.paper-btn-tapestry {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #5364cc;
  color: white !important;
  font-size: 20px;
  width: 200px;
  font-weight: 600;
}

.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}

.paper-btn:hover {
    opacity: 0.85;
}

.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}

.venue {
    font-size: 23px;
}

.topnav {
    background-color: #EEEEEE;
    overflow: hidden;
}

.topnav div {
    max-width: 1070px;
    margin: 0 auto;
}

.topnav a {
    display: inline-block;
    color: black;
    text-align: center;
    vertical-align: middle;
    padding: 16px 16px;
    text-decoration: none;
    font-size: 18px;
}

.topnav img {
    padding: 2px 0px;
    width: 100%;
    margin: 0.2em 0px 0.3em 0px;
    vertical-align: middle;
}

pre {
    font-size: 0.9em;
    padding-left: 7px;
    padding-right: 7px;
    padding-top: 3px;
    padding-bottom: 3px;
    border-radius: 3px;
    background-color: rgb(235, 235, 235);
    overflow-x: auto;
}

.download-thumb {
    display: flex;
}

@media only screen and (max-width: 620px) {
    .download-thumb {
        display: none;
    }
}

.paper-stuff {
    width: 50%;
    font-size: 20px;
}

@media only screen and (max-width: 620px) {
    .paper-stuff {
        width: 100%;
    }
}
* {
  box-sizing: border-box;
}

.column {
  text-align: center;
  float: left;
  width: 16.666%;
  padding: 5px;
}
.column3 {
  text-align: center;
  float: left;
  width: 33.333%;
  padding: 5px;
}
.column4 {
  text-align: center;
  float: left;
  width: 50%;
  padding: 5px;
}
.column5 {
  text-align: center;
  float: left;
  width: 20%;
  padding: 5px;
}
.column10 {
  text-align: center;
  float: left;
  width: 10%;
  padding: 5px;
}
.border-right {
    border-right: 1px solid black;
}
.border-bottom{
    border-bottom: 1px solid black;
}


.row-center {
    margin: 16px 0px 16px 0px;
    text-align: center;
}

/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
}
.img-fluid {
  max-width: 100%;
  height: auto;
}
.figure-img {
  margin-bottom: 0.5rem;
  line-height: 1;
}

.rounded-circle {
  border-radius: 50% !important;
}

/* Responsive layout - makes the three columns stack on top of each other instead of next to each other */
@media screen and (max-width: 500px) {
  .column {
    width: 100%;
  }
}
@media screen and (max-width: 500px) {
  .column3 {
    width: 100%;
  }
}

</style>

<script type="text/javascript"></script>
    <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
    <head>
        <title> AGILR </title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:description" content="A comprehensive tool utilization benchmark"/>
        <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
        <link rel="icon" href = "https://images.emojiterra.com/google/noto-emoji/unicode-16.0/color/svg/1f9e9.svg">
    </head>

 <body>

<div class="container">
    <div class="paper-title">
    <h1> 
        Agentic Jigsaw Interaction Learning for Enhancing <br> Visual Perception and Reasoning in Vision-Language Models
    </div>

    <div id="authors">
        <center>
            <div class="author-row-new">
                <a href="https://scholar.google.com.hk/citations?user=XJmAr8EAAAAJ&hl=zh-CN">Yu Zeng<sup>1*</sup></a>,
                <a href="https://scholar.google.com.hk/citations?user=6Ys6HgsAAAAJ&hl=zh-CN">Wenxuan Huang<sup>3,4*</sup></a>,
                Shiting Huang<sup>1*</sup>,
                Xikun Bao<sup>1</sup>,
                Yukun Qi<sup>1</sup>,
                Yiming Zhao<sup>1</sup>,
                Qiuchen Wang<sup>1</sup>,
                <a href="https://scholar.google.com.hk/citations?user=-t92FH8AAAAJ&hl=zh-CN">Lin Chen<sup>1</sup></a>,
                <a href="https://scholar.google.com.hk/citations?user=NfSsLncAAAAJ&hl=zh-CN">Zehui Chen<sup>1</sup></a>,
                Huaian Chen<sup>1</sup>,
                <a href="https://scholar.google.com.hk/citations?user=pw_0Z_UAAAAJ&hl=zh-CN">Wanli Ouyang<sup>2,4</sup></a>,
                <a href="https://scholar.google.com.hk/citations?user=r6CvuOUAAAAJ&hl=zh-CN">Feng Zhao<sup>1â€ </sup></a>
            </div>
        </center>
        <center>
        <div class="affiliations">
            <span><sup>1</sup> University of Science and Technology of China</span><br>
            <span><sup>2</sup> Shanghai AI Laboratory</span><br>
            <span><sup>3</sup> East China Normal University</span><br>
            <span><sup>4</sup> The Chinese University of Hong Kong</span>
        </div>

        <!-- <div class="affil-row">
            <div class="venue text-center"><b>NeurlIPS 2023 </b></div>
        </div> -->

        </center>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="paper-btn" href="https://arxiv.org/abs/2510.01304">
                <span class="material-icons"> description </span> 
                 Paper
            </a>
            <!-- <a class="paper-btn" href="https://colab.research.google.com/drive/1jvlzWMc6oo-TH1fYMl6hsOYfrcQj2rEs?usp=sharing">
                <span class="material-icons"> code </span> 
                 Colab
            </a>
            <a class="paper-btn-tapestry" href="https://colab.research.google.com/github/yilundu/reduce_reuse_recycle/blob/main/notebooks/image_tapestry.ipynb">
                <span class="material-icons"> code </span> 
                 Tapestry Colab
            </a> -->
            <a class="paper-btn" href="https://github.com/yuzeng0-0/AGILE">
                <span class="material-icons"> code </span>
                Code
            </a>
            <a class="paper-btn" href="https://huggingface.co/datasets/YuZeng260/AGILE">
                <span class="material-icons"> description </span> 
                 Data
            </a>
            <!-- <a class="paper-btn" href="./leaderboard.html">
                <span class="material-icons"> description </span> 
                 Leaderboard (EN)
            </a>
            <a class="paper-btn" href="./leaderboard_zh.html">
                <span class="material-icons"> description </span> 
                 Leaderboard (ZH)
            </a> -->
            </div>
        </div>
    </div>
    <p></p>
    <section id="abstract"/>
        <h2 style="text-align: center;">Abstract</h2>
        <p></p>
        <div class="flex-row" style="width: 75%; margin: 0 auto;">
            <p>
                Although current large Vision-Language Models (VLMs) have advanced in multimodal understanding and reasoning, their fundamental perceptual and reasoning abilities remain limited. Specifically, even on simple jigsaw tasks, existing VLMs perform near randomly, revealing deficiencies in core perception and reasoning capabilities. While high-quality vision-language data can enhance these capabilities, its scarcity and limited scalability impose significant constraints. To address this, we propose <strong>AGILE</strong>, an <strong>A</strong>gentic ji<strong>G</strong>saw <strong>I</strong>nteraction <strong>L</strong>earning for <strong>E</strong>nhancing visual perception and reasoning in VLMs. <strong>AGILE</strong> formulates jigsaw solving as an interactive process, enabling the model to progressively engage with the environment. At each step, the model generates executable code to perform an action based on the current state, while the environment provides fine-grained visual feedback to guide task completion. Through this iterative cycle of observation and interaction, the model incrementally improves its perceptual and reasoning capabilities via exploration and feedback. Experimental results show that <strong>AGILE</strong> not only substantially boosts performance on jigsaw tasks of varying complexity (e.g., increasing accuracy from 9.5% to 82.8% under the 2 &#215 2 setting) but also demonstrates strong generalization across 9 general vision tasks, achieving an average improvement of 3.1%. These results indicate notable enhancements in both perceptual and reasoning abilities. This work opens a new avenue for advancing reasoning and generalization in multimodal models and provides an efficient, scalable solution to the scarcity of multimodal reinforcement learning data.
            </p>
        </div>
    </section>
    <p></p>
    <section id="teaser-image">
        <hr>
        <center>
            <!-- <figure>
                <video class="centered" width="80%" autoplay loop muted playsinline class="video-background " >
                    <source src="materials/teaser.m4v" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </figure> -->
            <figure>
                <a>
                    <img width="80%" src="figure/main2.png"> 
                </a>
                <p class="caption", style="text-align: center;">
                    Figure 1. Description of the action space.
                </p>
            </figure>
            <figure>
                <a>
                    <img width="80%" src="figure/main1.png"> 
                </a>
                <p class="caption", style="text-align: center;">
                    Figure 2. Overview of <b>AGILE</b>.
                </p>
            </figure>
        </center>
    </section>
<p></p>
    
    <section id="Experiment"/>
        <h2 style="text-align: center;">Experiment</h2>
            <p></p>
             <p></p>
              <p></p>
        <hr>
        <center>
            <!-- <figure>
                <video class="centered" width="80%" autoplay loop muted playsinline class="video-background " >
                    <source src="materials/teaser.m4v" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </figure> -->
            <figure>
                <p class="caption", style="text-align: left;">
                    On average, <b>AGILE surpasses the base model Qwen2.5-VL-7B by 3.1% across all 9 benchmarks</b>, providing compelling evidence that jigsaw-based training effectively enhances the modelâ€™s ability to capture visual relations and strengthen reasoning skills, thereby improving its performance on general vision downstream tasks.
                </p>
                <a>
                    <img width="80%" src="figure/main_result.png"> 
                </a>
            </figure>
            <figure>
                <p class="caption", style="text-align: left;">
                    After supervised fine-tuning and reinforcement learning, Qwen2.5-VL-7B achieves substantial improvements on the jigsaw task, significantly <b>surpassing Gemini2.5-Pro and Qwen2.5-VL-72B</b>. These results demonstrate that modeling jigsaw solving as an interactive multi-turn dialogue effectively enhances visual perception and reasoning.
                </p>
                <a>
                    <img width="80%" src="figure/jigsaw_acc.png"> 
                </a>
            </figure>
            <figure>
                <p class="caption", style="text-align: left;">
                    We showcase several jigsaw-solving behaviors, including interpreting individual pieces, validating edge alignment through cropping and zooming, and reasoning about semantic consistency. <b>These human-like strategies highlight the emergence of advanced perceptual and reasoning capabilities.</b>
                </p>
                <a>
                    <img width="80%" src="figure/main3.png"> 
                </a>
            </figure>
        </center>
    </section>   



    <section id="reference"/>
    <hr>
    <h2 style="">Citation</h2>
    <pre>
<code>
TODO
</code>
    </pre>
    </section>   

</div>
</body>
</html>
